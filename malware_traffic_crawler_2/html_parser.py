# coding=utf-8
import re
from bs4 import BeautifulSoup
import urlparse
import os,urllib2,urllib

class HtmlParser(object):

    def __init__(self,file_path):
        #self.path = r'/home/test_LDG/malware-traffic-spider'
        self.path = file_path
        self.file_name = r'1.pcap.zip'  # 文件名，包含文件格式
        self.dest_dir = os.path.join(self.path, self.file_name)

    def parse(self, page_url, html_doc):
        if page_url is None or html_doc is None:
            return

        soup = BeautifulSoup(html_doc, 'html.parser', from_encoding='utf8')

        # 获得本网页中新的url
        new_urls = self._get_new_urls(page_url, soup)
        # 获得本网页中需要爬取得到的信息
        new_data = self._get_new_data(page_url, soup)

        return new_urls, new_data

    def _get_new_urls(self, page_url, soup):
        new_urls = set()

        # 要匹配的节点<a target="_blank" href="/view/2561555.htm">计算机程序设计语言</a>
        pat = re.compile(r"\d+/\d+/index\d*\.html")
        links = soup.find_all('a', href=pat)
        for link in links:
            new_url = link['href'] # 获得节点的href属性值, /view/2561555.htm
            new_full_url = urlparse.urljoin(page_url, new_url) #拼接成完整的url
            new_urls.add(new_full_url)

        return new_urls

    def _get_new_data(self, page_url, soup):

        new_data = {}
        new_data['url'] = page_url

        try:
            # 匹配<div class="lemma-summary" label-module="lemmaSummary">节点,获得summary
            pat = re.compile(r"(.)+\.pcap\.zip")
            links = soup.find_all('a', href=pat)
            #link = links[0]
            #因为每个网页li同一个*.pcap.zip包会有多个下载链接（页头和页尾），故用循环做一个过滤
            temp=[]
            for chr_link in links:
                new_url = chr_link['href']
                if new_url!=temp:
                    self.file_name = new_url
                    self.dest_dir = os.path.join(self.path, self.file_name)
                    new_full_url = urlparse.urljoin(page_url, new_url)
                    #urllib.urlretrieve(new_full_url, self.dest_dir)
                    self.downLoadPicFromURL(new_full_url)
                    temp=new_url
                else:
                    continue
        except:
            pass

        summary_node = soup.find('head').find('title')
        new_data['title'] = summary_node.get_text() # 获得summary_node的文字信息

        return new_data

    # 定义下载函数downLoadPicFromURL（本地文件夹，网页URL）
    def downLoadPicFromURL(self, url):
        try:
            urllib.urlretrieve(url, self.dest_dir)
        except Exception as e:
            print '\tError retrieving the URL:', url, e
