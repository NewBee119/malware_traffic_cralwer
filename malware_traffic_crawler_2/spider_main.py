# coding=utf-8
"""
这是调度程序，爬取步骤：
（１）调用url管理器获取需要爬取的url；
（２）通过html下载器下载网页；
（３）由html解析器解析获得url网页中需要的信息和新的url集合，以便后续爬取；
（４）收集爬取到的信息，生成html文件，进行爬取结果展示
"""
#import yara
import os
import url_manager
import html_downloader
import html_parser
import html_outputer

class SpiderMain(object):

    def __init__(self,file_path):
        self.urls = url_manager.UrlManager() # Url管理器
        self.downloader = html_downloader.HtmlDownloader()
        self.parser = html_parser.HtmlParser(file_path)
        self.outputer = html_outputer.HtmlOutputer()

    def craw(self, root_url):
        count = 1
        self.urls.add_new_url(root_url)

        while self.urls.has_new_url():
            try: # 由于有些url已经失效，进行下载会异常，故而进行异常捕获
                # 从url管理器中获取一个新的，待爬取的url
                new_url = self.urls.get_new_url()
                print "crawing {0} : {1}".format(count, new_url)
                # 调用html下载器，下载new_url对应的html的内容
                html_doc = self.downloader.download(new_url)
                # 调用html解析器，解析html内容，获取到网页内容中新的url和需要的信息
                new_urls, new_data = self.parser.parse(new_url, html_doc)
                # 将新的url存储到url管理器中
                self.urls.add_new_urls(new_urls)
                # 由outputer收集本次爬取，解析得到的网页信息
                self.outputer.collect_data(new_data)

                count += 1
            except:
                print '--- craw falled! ---'

        print 'craw finished!'
        # 爬取完毕，调用outputer显示爬取的结果
        self.outputer.output_html()


# 编写模块的主入口
if __name__ == '__main__':
    # 爬虫开始爬取的根url
    root_urls=['http://www.malware-traffic-analysis.net/2012/index.html']
    file_path=r'/Users/jihong/Downloads/pcap_new'
    file_num=2012
    for root_url in root_urls:
        curr_filePath=os.path.join('%s/%s' % (file_path,file_num))
        obj_spider = SpiderMain(curr_filePath) # 实例化一个爬虫对象
        obj_spider.craw(root_url) # 启动爬取信息
        file_num+=1
