#!/usr/bin/env python
# coding=utf-8
import threading
import download_request
import download_thread
import download_file_info
import settings
import math
import ssl
import os, re, requests
from lxml import etree
import urlparse
ssl._create_default_https_context = ssl._create_unverified_context

"""
a class for download file from network
"""
class ThunderDown:
    def __init__(self, url, path):
        self.downloadUrl = url
        self.file_info = download_request.get_netfile_info(self.downloadUrl)
        if not self.file_info:
            raise Exception('sorry,download can\'t start because resources is unreachable')
        self.path = path
        self.file_abspath = self.generate_file_name()
        self.lock = threading.Lock()
        self.__initThreads__()

    def __initThreads__(self):
        self.downloadThreads = []

        if self.file_info.file_size == download_file_info.NO_SIZE:
            thread_size = settings.MAX_THREAD_SIZE
            thread_capacity = settings.DEFAULT_THREAD_CAPACITY
        else:
            thread_capacity = settings.DEFAULT_THREAD_CAPACITY
            thread_size = int(
                min(math.ceil(self.file_info.file_size.to_kb() / float(thread_capacity)), settings.MAX_THREAD_SIZE))
            if thread_size == settings.MAX_THREAD_SIZE:
                thread_capacity = math.ceil(
                    self.file_info.file_size.to_kb() / float(thread_size))  # recalculate the thread download capacity

        start_pos = 0
        print('thread_size:', thread_size, ",thread_capacity:", thread_capacity)
        for i in range(thread_size):
            normal_end_pos = start_pos + thread_capacity - 1
            end_pos = min(normal_end_pos, self.file_info.file_size.to_kb() - 1)
            new_download_thread = download_thread.DownloadThread(self.downloadUrl, start_pos, end_pos)
            start_pos += thread_capacity
            self.downloadThreads.append(new_download_thread)

    """start download file"""

    def start_download(self):
        fileconn = open(self.file_abspath, self.judge_open_mode(self.downloadUrl))
        for download_thread in self.downloadThreads:
            download_thread.set_output(fileconn)
            download_thread.start()
        for download_thread in self.downloadThreads:
            download_thread.join()
        if fileconn:
            fileconn.close()

    """return completed size"""

    def get_total_downloaded(self):
        total_size = 0
        for thread in self.downloadThreads:
            total_size += thread.get_completed_len
        return total_size

    """pause download to wait restart"""

    def pause_download(self):
        pass

    """cancel the download anyway"""

    def cancel_download(self):
        pass

    """restart the paused download"""

    def restart_download(self):
        pass

    """invoked when the download finished"""

    def on_finish(self):
        pass

    """generate the downloadfilename"""

    def generate_file_name(self):
        default_file_path = self.path
        if not os.path.exists(default_file_path):
            os.mkdir(default_file_path)
        default_file_name = self.file_info.file_name
        file_path = os.path.join(default_file_path, default_file_name)
        max_try = 10
        if os.path.exists(file_path):
            file_name = file_path
            suffix_index = file_path.find('.')
            suffix = ''
            if suffix_index != -1:
                file_name = file_path[0:suffix_index]
                suffix = file_path[suffix_index + 1:]
            for i in range(max_try):
                if not len(suffix) == 0:
                    new_file_path = file_name + "(" + str(i + 1) + ")" + "." + suffix
                else:
                    new_file_path = file_name + "(" + str(i + 1) + ")"
                if not os.path.exists(new_file_path):
                    return new_file_path
                if i == max_try - 1:
                    os.remove(new_file_path)
                    return new_file_path
        else:
            return file_path

    """judge mode to open file by the download file's type of the url"""

    def judge_open_mode(self, url):
        return "wb"

if __name__ == '__main__':
    import M2Crypto
    import ssl

    cert = ssl.get_server_certificate(('www.baidu.com', 443))
    c = ssl.DER_cert_to_PEM_cert(cert)
    import base64
    cert = cert.replace('-----END CERTIFICATE-----','')
    cert = cert.replace('-----BEGIN CERTIFICATE-----','')
    s = base64.b64decode(cert)
    x509 = M2Crypto.X509.load_cert_string(c)

    spider_list = []
    downloaded = []
    try:
        with open('/Users/jihong/Desktop/tls_parser/spider/list.txt', 'r') as f:
            spider_list.extend(map(lambda x: x[0:-1], f.readlines()))
        with open('/Users/jihong/Desktop/tls_parser/spider/downloaded.txt', 'r') as f:
            downloaded.extend(f.readlines())
    except:
        pass
    for d in downloaded:
        if bool(re.match(r'^.*extract_files.*', d, re.IGNORECASE)):
            downloaded.remove(d)

    with open('/Users/jihong/Desktop/tls_parser/spider/downloaded.txt', 'w') as down_list:
            down_list.writelines(downloaded)
    # root_path = r'/Volumes/DataSet/CTU'
    # root_url = 'https://www.stratosphereips.org/datasets-malware/'
    # s = requests.session()
    # s.proxies = {"https": "127.0.0.1:6152", "http": "127.0.0.1:6152"}
    # html = etree.HTML(s.get(root_url).content)
    # tags_a = html.xpath('//*[@id="block-584f1549dd5c36a0e23f"]/div/p/a')
    # urls = []
    # for a in tags_a:
    #     if bool(re.match('CTU-Malware-Capture-Botnet', a.text, re.IGNORECASE)):
    #         urls.append((a.text, a.get('href')))
    # #urls.reverse()
    # for i, value in enumerate(urls):
    #     path, url = value
    #     if url in spider_list:
    #         continue
    #     file_path = os.path.join(root_path, path)
    #     if not os.path.exists(file_path):
    #         os.makedirs(file_path)
    #     content = etree.HTML(requests.get(url, verify=False).content)
    #     files = content.xpath('//tr/td[2]/a')
    #     for f in files[1:]:
    #         if f.text.endswith('/'):
    #             urls.insert(i + 1, (os.path.join(path, f.text), urlparse.urljoin(url, f.text)))
    #         else:
    #             try:
    #                 downloaded_url = urlparse.urljoin(url, f.text)
    #                 downloaded.append(downloaded_url)
    #                 with open('/Users/jihong/Desktop/tls_parser/spider/downloaded.txt', 'a+') as down_list:
    #                     down_list.write(downloaded_url + '\n')
    #             except Exception as e:
    #                 print(e)
    #     spider_list.append(url)
    #     with open('/Users/jihong/Desktop/tls_parser/spider/list.txt', 'a+') as spi_list:
    #             spi_list.write(url + '\n')
